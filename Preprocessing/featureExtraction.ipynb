{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\Laiba Aslam\\AppData\\Local\\Temp\\ipykernel_11384\\642718035.py:1: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  data = pd.read_json('E:\\Post ADP\\Capstone Project\\FYP\\datasets\\Cleaned_data_tokenized.json')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('E:\\Post ADP\\Capstone Project\\FYP\\datasets\\Cleaned_data_tokenized.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = data['Review Text']\n",
    "review_title = data['Review Title']\n",
    "description = data['Description']\n",
    "avg_rating = data['Average Rating']\n",
    "features = data['Features']\n",
    "Category = data['Category']\n",
    "review_rating = data['Review Rating']\n",
    "review_date = data['Review Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    using, Ninja, Call, Pro, Plus, smartwatch, mus...\n",
      "1    using, Ninja, Call, Pro, Plus, smartwatch, mus...\n",
      "2    Overall, product, good, two, things, would, li...\n",
      "3    Affordable, come, variety, features, including...\n",
      "4    call, functionality, seamless, clear, audio, e...\n",
      "Name: Review Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(review_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '14-Oct-24' does not match format '%d-%b-%y %H:%M:%S'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m\n\u001b[0;32m     78\u001b[0m     features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m review_length\n\u001b[0;32m     81\u001b[0m     extracted_features\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[1;32m---> 83\u001b[0m     review_timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mb-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43my \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(review_timestamp)\n\u001b[0;32m     85\u001b[0m features_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(extracted_features)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\_strptime.py:554\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    552\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    556\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\_strptime.py:333\u001b[0m, in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    331\u001b[0m found \u001b[38;5;241m=\u001b[39m format_regex\u001b[38;5;241m.\u001b[39mmatch(data_string)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found:\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime data \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match format \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    334\u001b[0m                      (data_string, \u001b[38;5;28mformat\u001b[39m))\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_string) \u001b[38;5;241m!=\u001b[39m found\u001b[38;5;241m.\u001b[39mend():\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconverted data remains: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    337\u001b[0m                       data_string[found\u001b[38;5;241m.\u001b[39mend():])\n",
      "\u001b[1;31mValueError\u001b[0m: time data '14-Oct-24' does not match format '%d-%b-%y %H:%M:%S'"
     ]
    }
   ],
   "source": [
    "# convert text into numerical form (bag of words model)\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "matrix = vectorizer.fit_transform(review_text, review_title)\n",
    "\n",
    "extracted_features = []\n",
    "\n",
    "for i, row in data.iterrows(): \n",
    "    review = row[\"Review Text\"].lower()\n",
    "    review_date = row['Review Date']  \n",
    "    \n",
    "\n",
    "    features = {}\n",
    "    # Polarity and subjectivity analysis (TextBlob model)\n",
    "    Sentiment = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = Sentiment.polarity_scores(review)\n",
    "    features['Positive_score'] = sentiment_score['pos']\n",
    "    features['negative_score'] = sentiment_score['neg']\n",
    "    features['neutral_score'] = sentiment_score['neu']\n",
    "    features['sentiment_score'] = sentiment_score\n",
    "    testimonial = TextBlob(review)\n",
    "    features['subjectivity'] = testimonial.sentiment.subjectivity\n",
    "    \n",
    "    # Word-level sentiment analysis\n",
    "    neg_word_count = 0\n",
    "    pos_word_count = 0\n",
    "    words = review.split()\n",
    "    for word in words:\n",
    "        word_polarity = TextBlob(word).sentiment.polarity\n",
    "        if word_polarity < 0:\n",
    "            neg_word_count += 1\n",
    "        elif word_polarity > 0:\n",
    "            pos_word_count += 1\n",
    "    features['Negative_Count'] = neg_word_count\n",
    "    features['Positive_Count'] = pos_word_count\n",
    "\n",
    "    #  Count the Articles and negations \n",
    "    articles = ['a', 'an', 'the']\n",
    "    negations = ['no', 'not', 'none', 'nobody', 'nothing',\n",
    "                 'neither', 'nowhere', 'never', 'hardly', 'barely', 'scarcely']\n",
    "    \n",
    "    Art = Nega = 0\n",
    "    for w in words:\n",
    "        if w in articles:\n",
    "            Art += 1\n",
    "        elif w in negations:\n",
    "            Nega += 1\n",
    "    features['Articles'] = Art\n",
    "    features['Negations'] = Nega\n",
    "\n",
    "    # Count the noun, adjective, verb, adverb, pronoun, unique words, word count, review length and authenticity score\n",
    "\n",
    "    Noun = Adj = Verb = Adv = Pro = 0\n",
    "    tags = pos_tag(words)\n",
    "    counts = Counter(tag for word, tag in tags)\n",
    "\n",
    "    Noun += sum([counts[i] for i in counts.keys() if 'NN' in i])\n",
    "    Adj += sum([counts[i] for i in counts.keys() if 'JJ' in i])\n",
    "    Verb += sum([counts[i] for i in counts.keys() if 'VB' in i])\n",
    "    Adv += sum([counts[i] for i in counts.keys() if 'RB' in i])\n",
    "    Pro += sum([counts[i] for i in counts.keys() if (('PRP' in i) or ('PRP$' in i) or ('WP' in i) or ('WP$' in i))])\n",
    "    unique_words_count = len(set(words))\n",
    "    word_count = len(words)\n",
    "    review_length = len(review)\n",
    "\n",
    "    if word_count > 0:\n",
    "        authenticity_score = (Pro + unique_words_count - neg_word_count) / word_count\n",
    "    else:\n",
    "        authenticity_score = 0 \n",
    "\n",
    "    features['Authenticity'] = authenticity_score\n",
    "    features['Noun'] = Noun\n",
    "    features['Adjectives'] = Adj\n",
    "    features['Verb'] = Verb\n",
    "    features['Adverb'] = Adv\n",
    "    features['Pronoun'] = Pro\n",
    "    features['Word_Count'] = word_count\n",
    "    features['Unique_words'] = unique_words_count\n",
    "    features['Review_Length'] = review_length\n",
    "\n",
    "    features['review_date'] = datetime.strptime(review_date, \"%d-%b-%y\")\n",
    "    extracted_features.append(features)\n",
    "\n",
    "    # print(review_timestamp)\n",
    "features_data = pd.DataFrame(extracted_features)\n",
    "\n",
    "# measure the similarity (TF-IDF and Cosine Similarity)\n",
    "\n",
    "def calculate_similarity(matrix1, matrix2, chunk_size=1000):\n",
    "    num_rows = matrix1.shape[0]\n",
    "    similarity_scores = np.zeros(num_rows)\n",
    "    \n",
    "    for start_idx in range(0, num_rows, chunk_size):\n",
    "        end_idx = min(start_idx + chunk_size, num_rows)\n",
    "        chunk_similarity = cosine_similarity(matrix1[start_idx:end_idx], matrix2).max(axis=1)\n",
    "        similarity_scores[start_idx:end_idx] = chunk_similarity\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "tfidf_reviews = tfidf_vectorizer.fit_transform(review_text)\n",
    "tfidf_description = tfidf_vectorizer.transform(description)\n",
    "tfidf_features = tfidf_vectorizer.transform(features)\n",
    "tfidf_categories = tfidf_vectorizer.transform(Category)\n",
    "\n",
    "features_data['similarity_with_description ']= calculate_similarity(tfidf_reviews, tfidf_description)\n",
    "features_data['similarity_with_features'] = calculate_similarity(tfidf_reviews, tfidf_features)\n",
    "features_data['similarity_with_category'] = calculate_similarity(tfidf_reviews, tfidf_categories)\n",
    "features_data['Review_Text'] = review_text\n",
    "\n",
    "\n",
    "print(features_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete! Processed file saved as: processed_data_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "output_file_path = \"processed_data_with_sentiment.csv\"\n",
    "features_data.to_csv(output_file_path)\n",
    "\n",
    "print(\"Feature extraction complete! Processed file saved as:\", output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
