{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download(\"popular\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Function\n",
    "def preprocess_text(review):\n",
    "    # Expand contractions\n",
    "    review = contractions.fix(review)\n",
    "    \n",
    "    # Remove non-word characters and digits\n",
    "    review = re.sub(r'\\W', ' ', review)\n",
    "    review = re.sub(r'\\d', ' ', review)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = review.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join back to a single string\n",
    "    cleaned_text = ', '.join(filtered_words)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Function\n",
    "def extract_features(cleaned_review):\n",
    "    # Sentiment Analysis\n",
    "    testimonial = TextBlob(cleaned_review)\n",
    "    Sentiment = testimonial.sentiment.polarity\n",
    "    Subjectivity = testimonial.sentiment.subjectivity\n",
    "    \n",
    "    # Negative Word Count\n",
    "    neg = 0\n",
    "    words = cleaned_review.split()\n",
    "    for w in words:\n",
    "        testimonial = TextBlob(w)\n",
    "        score = testimonial.sentiment.polarity\n",
    "        if score < 0:\n",
    "            neg += 1\n",
    "     # Word Statistics\n",
    "    word_count = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    \n",
    "    # Part-of-Speech (POS) Tagging and Counts\n",
    "    tokenized = sent_tokenize(cleaned_review)\n",
    "    Noun = Adj = Verb = Adv = Pro = Pre = Con = Art = Aux = 0\n",
    "    for sentence in tokenized:\n",
    "        words_list = nltk.word_tokenize(sentence)\n",
    "        tagged = pos_tag(words_list)\n",
    "        counts = Counter(tag for word, tag in tagged)\n",
    "        Noun += sum([counts[i] for i in counts.keys() if 'NN' in i])\n",
    "        Adj += sum([counts[i] for i in counts.keys() if 'JJ' in i])\n",
    "        Verb += sum([counts[i] for i in counts.keys() if 'VB' in i])\n",
    "        Adv += sum([counts[i] for i in counts.keys() if 'RB' in i])\n",
    "    \n",
    "    # Custom Metrics\n",
    "    authenticity = (unique_words - neg) / word_count\n",
    "    data_stat = {\n",
    "        'Sentiment': Sentiment,\n",
    "        'Subjectivity': Subjectivity,\n",
    "        'Negative_Count': neg,\n",
    "        'Word_Count': word_count,\n",
    "        'Unique_Words': unique_words,\n",
    "        'Noun_Count': Noun,\n",
    "        'Adjective_Count': Adj,\n",
    "        'Verb_Count': Verb,\n",
    "        'Adverb_Count': Adv,\n",
    "        'Authenticity': authenticity\n",
    "    }\n",
    "    return data_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to E:\\Post ADP\\Capstone Project\\Scrapping from Amazon\\datasets\\Processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "input_file = r\"E:\\Post ADP\\Capstone Project\\Scrapping from Amazon\\datasets\\Cleaned_data.csv\"\n",
    "output_file = r\"E:\\Post ADP\\Capstone Project\\Scrapping from Amazon\\datasets\\Processed_data.csv\"\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(input_file, encoding='latin1')\n",
    "\n",
    "# Create a list to store the processed data\n",
    "processed_data = []\n",
    "\n",
    "# Specify the columns to process\n",
    "columns_to_process = ['Review Text', 'productTitle', 'Description', 'Features', 'Review Title', 'Category']\n",
    "\n",
    "# Process each review in the dataset\n",
    "for index, row in df.iterrows():\n",
    "    combined_review = \" \".join([str(row[col]) for col in columns_to_process if col in df.columns])\n",
    "    cleaned_review = preprocess_text(combined_review)\n",
    "    features = extract_features(cleaned_review)\n",
    "    features['Original_Review'] = combined_review\n",
    "    features['Cleaned_Review'] = cleaned_review\n",
    "    processed_data.append(features)\n",
    "\n",
    "# Convert processed data into a DataFrame\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "processed_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Review Title\": \"Good quality\",\n",
    "    \"Review Text\": \"The charger is good. However, it should be narrow from the front so that it can fit in the power sockets.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['be' 'can' 'charger' 'fit' 'from' 'front' 'good' 'however' 'in' 'is' 'it'\n",
      " 'narrow' 'power' 'quality' 'should' 'so' 'sockets' 'that' 'the']\n",
      "Bag of Words Representation:\n",
      "[[0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 2 1 1 0 1 1 1 1 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Combine the text data (Review Title + Review Text)\n",
    "text_data = [\n",
    "    data[\"Review Title\"],\n",
    "    data[\"Review Text\"]\n",
    "]\n",
    "\n",
    "# Initialize the Bag of Words vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer and transform the text into a Bag of Words representation\n",
    "bow_features = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Convert to array for readability\n",
    "bow_array = bow_features.toarray()\n",
    "\n",
    "# Get the feature names (words in the vocabulary)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the results\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Bag of Words Representation:\")\n",
    "print(bow_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Read the cleaned reviews from the file\n",
    "with open(\"E:\\Post ADP\\Capstone Project\\Scrapping from Amazon\\datasets\\Cleaned_data_tokenized.json\", \"r\") as file:\n",
    "    reviews = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Define helper function to extract features\n",
    "def extract_features_from_tokens(tokens):\n",
    "    # Part-of-speech tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Calculate features\n",
    "    num_unique_words = len(set(tokens))  # Number of unique words\n",
    "    positive_words = sum(1 for word in tokens if word in opinion_lexicon.positive())  # Positive words\n",
    "    negative_words = sum(1 for word in tokens if word in opinion_lexicon.negative())  # Negative words\n",
    "    num_nouns = sum(1 for word, pos in pos_tags if pos.startswith('NN'))  # Nouns\n",
    "    num_adjectives = sum(1 for word, pos in pos_tags if pos.startswith('JJ'))  # Adjectives\n",
    "    num_adverbs = sum(1 for word, pos in pos_tags if pos.startswith('RB'))  # Adverbs\n",
    "    num_verbs = sum(1 for word, pos in pos_tags if pos.startswith('VB'))  # Verbs\n",
    "    review_length = len(tokens)  # Length of the review\n",
    "    subjectivity = TextBlob(\" \".join(tokens)).sentiment.subjectivity  # Subjectivity score (0-1)\n",
    "    authenticity = positive_words - negative_words  # Simplified authenticity score\n",
    "    \n",
    "    # Return feature dictionary\n",
    "    return {\n",
    "        \"Unique Words\": num_unique_words,\n",
    "        \"Positive Words\": positive_words,\n",
    "        \"Negative Words\": negative_words,\n",
    "        \"Nouns\": num_nouns,\n",
    "        \"Adjectives\": num_adjectives,\n",
    "        \"Adverbs\": num_adverbs,\n",
    "        \"Verbs\": num_verbs,\n",
    "        \"Review Length\": review_length,\n",
    "        \"Subjectivity\": subjectivity,\n",
    "        \"Authenticity\": authenticity\n",
    "    }\n",
    "\n",
    "# Process each review\n",
    "features_list = []\n",
    "for review in reviews:\n",
    "    tokens = review.split()  # Split the cleaned review into tokens\n",
    "    features = extract_features_from_tokens(tokens)\n",
    "    features_list.append(features)\n",
    "\n",
    "# Save features to a new file\n",
    "with open(\"review_features.json\", \"w\") as file:\n",
    "    for idx, features in enumerate(features_list, start=1):\n",
    "        file.write(f\"Review {idx}: {features}\\n\")\n",
    "\n",
    "# Print features\n",
    "for idx, features in enumerate(features_list, start=1):\n",
    "    print(f\"Review {idx}: {features}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
